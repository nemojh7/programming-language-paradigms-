{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0af7ecd",
      "metadata": {
        "id": "c0af7ecd"
      },
      "source": [
        "# 90:10(10-test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37000554",
      "metadata": {
        "id": "37000554",
        "outputId": "e5372822-eb94-4b29-a5a1-a00ba1c57f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "# 키워드 기반 라벨 할당\n",
        "def assign_label(content, keywords):\n",
        "    keyword_counts = {keyword: content.count(keyword) for keyword in keywords}\n",
        "    assigned_label = max(keyword_counts, key=keyword_counts.get, default='unknown')\n",
        "    return assigned_label if keyword_counts[assigned_label] > 0 else 'unknown'\n",
        "\n",
        "# 파일 경로 설정\n",
        "folder_path = r'C:/Users/User/Python_data_3000'\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "corpus = []  # 코퍼스 초기화\n",
        "labels = []  # 라벨 초기화\n",
        "\n",
        "# 파일별 데이터 읽어오기 및 라벨 할당\n",
        "for file_path in file_paths:\n",
        "    content = read_data(file_path)\n",
        "    corpus.append(content)\n",
        "    labels.append(assign_label(content, keywords))\n",
        "\n",
        "# TF-IDF로 특징 추출\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, test_size=0.1, random_state=42, stratify=labels)\n",
        "\n",
        "# SVM 모델 학습\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도, 정밀도, 재현율, F1-score 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a71ec0",
      "metadata": {
        "id": "45a71ec0"
      },
      "source": [
        "# 80:20(20-test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf83da70",
      "metadata": {
        "id": "cf83da70",
        "outputId": "dd9a4f87-5134-423c-8977-1ceec16f2bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9983333333333333\n",
            "Precision: 0.9966694444444445\n",
            "Recall: 0.9983333333333333\n",
            "F1-score: 0.9975006950236309\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "# 키워드 기반 라벨 할당\n",
        "def assign_label(content, keywords):\n",
        "    keyword_counts = {keyword: content.count(keyword) for keyword in keywords}\n",
        "    assigned_label = max(keyword_counts, key=keyword_counts.get, default='unknown')\n",
        "    return assigned_label if keyword_counts[assigned_label] > 0 else 'unknown'\n",
        "\n",
        "# 파일 경로 설정\n",
        "folder_path = r'C:/Users/User/Python_data_3000'\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "corpus = []  # 코퍼스 초기화\n",
        "labels = []  # 라벨 초기화\n",
        "\n",
        "# 파일별 데이터 읽어오기 및 라벨 할당\n",
        "for file_path in file_paths:\n",
        "    content = read_data(file_path)\n",
        "    corpus.append(content)\n",
        "    labels.append(assign_label(content, keywords))\n",
        "\n",
        "# TF-IDF로 특징 추출\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# SVM 모델 학습\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도, 정밀도, 재현율, F1-score 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "360455e5",
      "metadata": {
        "id": "360455e5"
      },
      "source": [
        "# 70:30(30-test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8a9b55",
      "metadata": {
        "id": "4f8a9b55",
        "outputId": "075af55f-3464-4aa7-f7ca-6a393a88bcc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "# 키워드 기반 라벨 할당\n",
        "def assign_label(content, keywords):\n",
        "    keyword_counts = {keyword: content.count(keyword) for keyword in keywords}\n",
        "    assigned_label = max(keyword_counts, key=keyword_counts.get, default='unknown')\n",
        "    return assigned_label if keyword_counts[assigned_label] > 0 else 'unknown'\n",
        "\n",
        "# 파일 경로 설정\n",
        "folder_path = r'C:/Users/User/Python_data_3000'\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "corpus = []  # 코퍼스 초기화\n",
        "labels = []  # 라벨 초기화\n",
        "\n",
        "# 파일별 데이터 읽어오기 및 라벨 할당\n",
        "for file_path in file_paths:\n",
        "    content = read_data(file_path)\n",
        "    corpus.append(content)\n",
        "    labels.append(assign_label(content, keywords))\n",
        "\n",
        "# TF-IDF로 특징 추출\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# SVM 모델 학습\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도, 정밀도, 재현율, F1-score 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9f3114",
      "metadata": {
        "id": "da9f3114"
      },
      "source": [
        "# 60:40(40-test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828945a7",
      "metadata": {
        "id": "828945a7",
        "outputId": "efdc6e0a-cc66-4f00-c77c-2da61b3eca04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9991666666666666\n",
            "Precision: 0.9983340277777777\n",
            "Recall: 0.9991666666666666\n",
            "F1-score: 0.9987501736834793\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "# 키워드 기반 라벨 할당\n",
        "def assign_label(content, keywords):\n",
        "    keyword_counts = {keyword: content.count(keyword) for keyword in keywords}\n",
        "    assigned_label = max(keyword_counts, key=keyword_counts.get, default='unknown')\n",
        "    return assigned_label if keyword_counts[assigned_label] > 0 else 'unknown'\n",
        "\n",
        "# 파일 경로 설정\n",
        "folder_path = r'C:/Users/User/Python_data_3000'\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "corpus = []  # 코퍼스 초기화\n",
        "labels = []  # 라벨 초기화\n",
        "\n",
        "# 파일별 데이터 읽어오기 및 라벨 할당\n",
        "for file_path in file_paths:\n",
        "    content = read_data(file_path)\n",
        "    corpus.append(content)\n",
        "    labels.append(assign_label(content, keywords))\n",
        "\n",
        "# TF-IDF로 특징 추출\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, test_size=0.4, random_state=42, stratify=labels)\n",
        "\n",
        "# SVM 모델 학습\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도, 정밀도, 재현율, F1-score 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d77a29",
      "metadata": {
        "id": "29d77a29"
      },
      "source": [
        "# 50:50(50-test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b8d6ad",
      "metadata": {
        "id": "e3b8d6ad",
        "outputId": "4728421f-8587-4f51-bac6-c1637776b706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9986666666666667\n",
            "Precision: 0.9973351111111112\n",
            "Recall: 0.9986666666666667\n",
            "F1-score: 0.9980004447409383\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 파일에서 데이터 읽어오기\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    return data\n",
        "\n",
        "# 키워드 기반 라벨 할당\n",
        "def assign_label(content, keywords):\n",
        "    keyword_counts = {keyword: content.count(keyword) for keyword in keywords}\n",
        "    assigned_label = max(keyword_counts, key=keyword_counts.get, default='unknown')\n",
        "    return assigned_label if keyword_counts[assigned_label] > 0 else 'unknown'\n",
        "\n",
        "# 파일 경로 설정\n",
        "folder_path = r'C:/Users/User/Python_data_3000'\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "corpus = []  # 코퍼스 초기화\n",
        "labels = []  # 라벨 초기화\n",
        "\n",
        "# 파일별 데이터 읽어오기 및 라벨 할당\n",
        "for file_path in file_paths:\n",
        "    content = read_data(file_path)\n",
        "    corpus.append(content)\n",
        "    labels.append(assign_label(content, keywords))\n",
        "\n",
        "# TF-IDF로 특징 추출\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, labels, test_size=0.5, random_state=42, stratify=labels)\n",
        "\n",
        "# SVM 모델 학습\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 정확도, 정밀도, 재현율, F1-score 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d67461e",
      "metadata": {
        "id": "4d67461e"
      },
      "source": [
        "# 나이브 베이지안(Naive Bayesian) 알고리즘"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed0b92b",
      "metadata": {
        "id": "7ed0b92b",
        "outputId": "b7359662-9d0e-4f00-f8b2-f0aa04d37779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00   1969279\n",
            "           1       0.73      0.93      0.82     43073\n",
            "\n",
            "    accuracy                           0.99   2012352\n",
            "   macro avg       0.87      0.96      0.91   2012352\n",
            "weighted avg       0.99      0.99      0.99   2012352\n",
            "\n",
            "Data chunk 1 saved to classified_questions_part_1.xlsx\n",
            "Data chunk 2 saved to classified_questions_part_2.xlsx\n",
            "Data chunk 3 saved to classified_questions_part_3.xlsx\n",
            "Data chunk 4 saved to classified_questions_part_4.xlsx\n",
            "Data chunk 5 saved to classified_questions_part_5.xlsx\n",
            "Data chunk 6 saved to classified_questions_part_6.xlsx\n",
            "Data chunk 7 saved to classified_questions_part_7.xlsx\n",
            "Data chunk 8 saved to classified_questions_part_8.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 문제 해결과 관련된 키워드 목록 정의\n",
        "problem_solving_keywords = ['error', 'debugging', 'algorithm', 'optimization']\n",
        "\n",
        "# 경로 설정\n",
        "directory_path = 'C:/Users/User/Python_data_3000'\n",
        "\n",
        "# 데이터 로드 및 레이블링\n",
        "def load_and_label_questions(directory, keywords):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            language = filename.split(\".txt\")[0]  # 프로그래밍 언어 식별\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                for line in file:\n",
        "                    line = line.strip().lower()\n",
        "                    label = 'no'\n",
        "                    for keyword in keywords:\n",
        "                        if keyword in line:\n",
        "                            label = keyword\n",
        "                            break\n",
        "                    data.append({\"language\": language, \"question\": line, \"label\": label})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# 나이브 베이지안 알고리즘을 사용하여 질문 분류\n",
        "def classify_questions(data):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(data['question'])\n",
        "    y = (data['label'] != 'no').astype(int)  # 문제 해결 관련 여부를 0, 1로 변환\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    classifier = MultinomialNB()\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return classifier, vectorizer\n",
        "\n",
        "# 엑셀 파일로 분할하여 저장\n",
        "def save_to_multiple_excel_files(data, chunk_size=1048576, base_filename='classified_questions_part'):\n",
        "    number_of_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)\n",
        "    for i in range(number_of_chunks):\n",
        "        start_index = i * chunk_size\n",
        "        end_index = start_index + chunk_size\n",
        "        chunk_data = data.iloc[start_index:end_index]\n",
        "        filename = f\"{base_filename}_{i+1}.xlsx\"\n",
        "        chunk_data.to_excel(filename, index=False)\n",
        "        print(f\"Data chunk {i+1} saved to {filename}\")\n",
        "\n",
        "# 메인 실행 함수\n",
        "def main():\n",
        "    data = load_and_label_questions(directory_path, problem_solving_keywords)\n",
        "    classifier, vectorizer = classify_questions(data)\n",
        "    save_to_multiple_excel_files(data)  # Modify this line to use the new function\n",
        "\n",
        "# 실행\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77eef3d3",
      "metadata": {
        "id": "77eef3d3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}